# Video Preprocess Code for Multimodal Conversational Research: Diarization, Segmentation and Transcription

This project is designed to process videos through a series of automated tasks including:

- **Audio Extraction & Speaker Diarization**: Extract audio from videos, perform speaker diarization, and generate clean timestamp files.
- **Video Segmentation**: Generate segmentation timestamps, cut the videos into clips based on these timestamps, and transcribe the clips using the Whisper model.

The project is organized into two main modules/steps:
- **process_videos**: Handles audio extraction, diarization, and timestamp denoise/merge.
- **video_segmentation**: Handles generating segmentation timestamps, video cutting, and transcription of video clips.

## ğŸ¦„ Project Structure
```bash
project/
â”œâ”€â”€ main.py
â”œâ”€â”€ check_speaker.py
â”œâ”€â”€ process_videos/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_extractor.py
â”‚   â”œâ”€â”€ diarization_processor.py
â”‚   â”œâ”€â”€ timestamp_processor.py
â”‚   â””â”€â”€ video_processor.py
â”œâ”€â”€ video_segmentation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ timestamp_generator.py
â”‚   â”œâ”€â”€ video_cutter.py
â”‚   â””â”€â”€ video_transcriber.py
â””â”€â”€ setup.py
```


## ğŸ”¨ Installation

### Prerequisites

- Python 3.10
- [PyTorch](https://pytorch.org/)  
- [pyannote.audio](https://github.com/pyannote/pyannote-audio)  
- [whisper](https://github.com/openai/whisper)  
- Other dependencies as listed in [setup.py](./setup.py) 

### Model download

1. [PyTorch](https://pytorch.org/)

You need to apply for access on [HuggingFace](https://huggingface.co/fatymatariq/speaker-diarization-3.1).
And you should specify your auth_token in [diarization_processor.py](./process_videos/diarization_processor.py#L6-L7).

2. [whisper](https://github.com/openai/whisper)

You can download Whisper-small in  [HuggingFace](https://huggingface.co/openai/whisper-small). (Or choose other size Whisper model as you need).
Then specify the model path in your argparse when you run "segment" command.

### Data Preparation
We recommend that you prepare your long video data in the format below:
```bash
video_data/
â”œâ”€â”€ 0001/
â”‚   â”œâ”€â”€ 0001.mp4
â”œâ”€â”€ 0002/
â”‚   â”œâ”€â”€ 0002.mp4
...
â”œâ”€â”€ 0455/
â”‚   â”œâ”€â”€ 0455.mp4
```
The ID numbers don't need to be continuous. If you have your customized data format, the code need to be changed by yourself.

### Get start

1. **Clone the Repository**

```bash
git clone 
cd project
```

2. **Install the Package**

From the project root, run:

```bash
pip install -e .
```

## ğŸª„ Usage
The project provides a unified command-line interface through [main.py](./main.py) with two subcommands.

### Step 1. Process Videos

This command will:

1. Extract audio from input videos (expecting subfolders named 0001 to 0454 with corresponding .mp4 files).

2. Perform speaker diarization on selected audio files.

3. Process and clean the generated timestamp files (denoise/merge).

```bash
python main.py process_videos /path/to/your/video_folder
```

Replace /path/to/your/video_folder with the directory containing your video subfolders.

### âš ï¸ Step 1.5. Mannual Check
We strongly recommand you have a mannual check in your merged timestamps in **data-time**, and edit unqualified .txt file by yourself.
Before that, you can use check_speaker.py to automatically output unqualified file names.

That will be very helpful to have right video clips!

### Step 2. Video Segmentation
This command will:

1. Generate segmentation timestamps from merged timestamp files.

2. Extract video clips based on the generated timestamps.

3. Transcribe the video clips using the Whisper model.

```bash
python main.py segment --video_base /path/to/your/video_folder --whisper_model /path/to/whisper-model/small.pt

```
Replace with the directory containing your video subfolders and whisper models.

### Generated Files
After step.1, you will get:
- data-audio: Seperated audio file (.wav) in each video.
- data-time: This diarization time is generated after diarization, denoise and merge. (eg. Speaker SPEAKER_00 from 0.0s to 2.8s)

Note: If you want to generate the original diarization time, feel free to customize the code.

After step.2, you will get:
- data-timestamps: Timestamps number to guide video cutting.
- data-clips: Video clips after video cutting. 
- data-text: Transcription generated by Whisper model (selected by users).


## âœï¸ Citation
If you find this work useful for your research, please feel free to leave a starâ­ï¸ and cite our paper:

```bibtex
@misc{yang2025saynext,
      title={SayNext: A Novel Benchmark for Multimodal Emotion Understanding via Next-Utterance Prediction}, 
      author={Yueyi Yang and Haotian Liu and Haoyu Chen},
      year={2025},
      eprint={arXiv:2403.xxxxx}, 
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## ğŸ¤ Acknowledgement
This work is supported by **The University of Oulu & The Research Council of Finland, PROFI7 352788**. Thanks to [Pyannote](https://github.com/pyannote/pyannote-audio) and [Whisper ](https://github.com/openai/whisper). 
